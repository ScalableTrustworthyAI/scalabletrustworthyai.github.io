<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7RQJFKWW"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RF7RQJFKWW")</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script>
<script src=https://ScalableTrustworthyAI.github.io/js/hugo-academic-group.js></script>
<link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/bootstrap.min.css><script src=https://ScalableTrustworthyAI.github.io/js/bootstrap.min.js></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css><script src=https://ScalableTrustworthyAI.github.io/js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/font-awesome.min.css><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/hugo-academic-group.css><link rel="shortcut icon" href=https://ScalableTrustworthyAI.github.io/img/favicon.png type=image/x-icon><link rel=canonical href=https://ScalableTrustworthyAI.github.io/courses/tml_winter_2223/><title>Trustworthy Machine Learning | Scalable Trustworthy AI</title></head><body><div class=home-anchor id=home></div><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://ScalableTrustworthyAI.github.io/><img src=https://ScalableTrustworthyAI.github.io/img/stai_logo.png alt="Research group logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#top>Home</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#overview>Overview</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#members>Members</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#openings>Openings</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#contact>Contact</a></li></ul></div></div></nav><div><br></div><div class=container><div class=row itemprop=author itemscope itemtype=http://schema.org/Person><div class="visible-sm visible-xs"></div><div class=col-sm-12 itemprop=description><h2>Trustworthy Machine Learning</h2><h4>Winter Semester 2022-2023, University of Tübingen</h4></div><div class=col-sm-8 itemprop=description><div class=article-style>As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalise well to small changes in the distribution; some models are found to utilise sensitive features that could treat certain demographic user groups unfairly; models tend to be confident on novel types of data; models cannot communicate the rationale behind their decisions effectively with the end users like medical staff to maximise the human-machine synergies. Collectively, we face a trustworthiness issue with the current machine learning technology. A large fraction of the machine learning research nowadays is dedicated to expanding the frontier of Trustworthy Machine Learning (TML). The course covers a theoretical and technical background for key topics in TML. We conduct a critical review of important classical and contemporary research papers on related topics and provide hands-on practicals to implement TML techniques.</div></div><div class=col-sm-4 itemprop=description><img src=https://ScalableTrustworthyAI.github.io/img/DALLE_TML.png class=course-banner itemprop=image></div></div><div class=row itemprop=author itemscope itemtype=http://schema.org/Person><div class=col-sm-12 itemprop=description><div class=course-article-style><h3 id=lecturer>Lecturer</h3><ul><li><a href=../../member/joon/>Seong Joon Oh</a></li></ul><h3 id=tutors>Tutors</h3><ul><li><a href=../../member/alex/>Alexander Rubinstein</a></li><li><a href=../../member/elif/>Elif Akata</a></li><li><a href=../../member/elisa/>Elisa Nguyen</a></li></ul><h3 id=central-email>Central email</h3><p>Please use the STAI group email <code>stai.there@gmail.com</code> to</p><ul><li>Submit your Colab exercises;</li><li>Ask questions; and</li><li>Send us feedback.</li></ul><h3 id=slack-forum>Slack forum</h3><p>Ask the lecturer or tutors to add you to the Slack channel. We need your name and email address. There, you can</p><ul><li>Ask questions and</li><li>Send us feedback.</li></ul><h3 id=goal>Goal</h3><ol><li>Students will be able to critically read, assess, and discuss research work in Trustworthy Machine Learning (TML).</li><li>Students will gain the technical background to implement basic TML techniques in a deep learning framework.</li><li>Students will be ready to conduct their own research in TML and make contributions to the research community.</li></ol><h3 id=prerequisites>Prerequisites</h3><ul><li>Familiarity with Python and PyTorch coding.</li><li>A pass grade from the Deep Learning Course (or equivalent).</li><li>Basic knowledge of machine learning concepts.</li><li>Basic maths: multivariate calculus, linear algebra, probability, statistics, and optimisation.</li></ul><h3 id=reading-list>Reading list</h3><p>Recommended papers from each topic.</p><details><summary>&#9656; OOD Generalisation (click to expand)</summary><ul><li>Generalizing from a Few Examples: A Survey on Few-Shot Learning</li><li>Generalizing to Unseen Domains: A Survey on Domain Generalization</li><li>In Search of Lost Domain Generalization</li><li>Underspecification Presents Challenges for Credibility in Modern Machine Learning</li><li>Shortcut Learning in Deep Neural Networks</li><li>Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective</li><li>Environment Inference for Invariant Learning</li><li>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</li></ul></details><details><summary>&#9656; Explainability (click to expand)</summary><ul><li>Explanation in artificial intelligence: Insights from the social sciences</li><li>Sanity Checks for Saliency Maps</li><li>A benchmark for interpretability methods in deep neural networks</li><li>Expanding Explainability: Towards Social Transparency in AI systems</li><li>Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach</li><li>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</li><li>Axiomatic attribution for deep networks</li><li>SmoothGrad: removing noise by adding noise</li><li>A Unified Approach to Interpreting Model Predictions</li><li>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</li><li>Visualizing Deep Neural Network Decisions: Prediction Difference Analysis</li><li>Learning Deep Features for Discriminative Localization</li><li>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</li><li>&ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier</li><li>Feature Visualization</li><li>Understanding Black-box Predictions via Influence Functions</li><li>Estimating Training Data Influence by Tracing Gradient Descent</li></ul></details><details><summary>&#9656; Uncertainty (click to expand)</summary><ul><li>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</li><li>On calibration of modern neural networks</li><li>Deep Anomaly Detection with Outlier Exposure</li><li>A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</li><li>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</li><li>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</li><li>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</li><li>Averaging weights leads to wider optima and better generalization</li><li>Efficient and scalable Bayesian neural nets with rank-1 factors</li><li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</li><li>Addressing Failure Prediction by Learning Model Confidence</li><li>Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks</li><li>Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</li><li>DiverseNet: When One Right Answer is not Enough</li></ul></details><details><summary>&#9656; Evaluation (click to expand)</summary><ul><li>Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets</li><li>A Metric Learning Reality Check</li><li>Evaluating Weakly-Supervised Object Localization Methods Right</li><li>Do ImageNet Classifiers Generalize to ImageNet?</li><li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.</li><li>Realistic Evaluation of Deep Semi-Supervised Learning Algorithms</li><li>Zero-Shot Learning-The Good, the Bad and the Ugly</li><li>Sanity Checks for Saliency Maps</li><li>In Search of Lost Domain Generalization</li><li>Are We Learning Yet? A Meta-Review of Evaluation Failures Across Machine Learning</li><li>How to avoid machine learning pitfalls: a guide for academic researchers</li><li>Show Your Work: Improved Reporting of Experimental Results</li></ul></details><h3 id=registration>Registration</h3><p>We can take only 60 students for the course. The registration is on a first-come-first-served basis.</p><h3 id=grading-policy>Grading policy</h3><p><img src=https://ScalableTrustworthyAI.github.io/img/grading_scheme.png alt=TML_grading></p><p>The final grade is essentially based on your exam performance. However, to encourage the participation in the exercises,
we award bonus boosts on the final grade based on the exercise performance.</p><p><strong>Exercise 0 and admission to course</strong></p><ul><li>We admit only those who submit the zeroth exercise to the course.</li><li>Exercise 0 is an individual exercise.</li></ul><p><strong>Exercise 1-4 score E% (0 - 100%)</strong></p><ul><li>Average score from Exercises 1-4.</li><li>Exercises 1-4 are group exercises.</li></ul><p><strong>Admission to exam</strong></p><ul><li>When the exercise grade is $>60$%, you are granted the right to participate in the exam.</li><li>Can take care of exceptional cases on an individual basis.</li></ul><p><strong>Exam score (0 - 100%)</strong></p><ul><li>Based on your performance on your exam.</li><li>The pass threshold for the course will be $\geq 50$% on the exam.</li><li>When you cannot attend the first exam or have failed the first one, you may take the <strong>make-up exam</strong>.</li></ul><p><strong>Final score (0 - 100%)</strong></p><ul><li>The final score is your exam score with possible increments from your exercise performance.</li><li>Increment from exercise = $(E-60)/40 \times 20$%, where $E$% is your exercise score.<ul><li>If $E=100$, you will get 20%p additional points on top of your exam grade.</li><li>If $E=60$, no additional points will be awarded.</li><li>If $E&lt; 60$, you will not be admitted to the exam.</li></ul></li><li>Increments from exercise will not let you pass the course if your exam score is below $50%$.</li></ul><p><strong>Final grade (1,0 - 5,0)</strong></p><ul><li>The final grade of the course will be based on the final score.</li></ul><h3 id=lecture-times>Lecture times</h3><ul><li>Fridays</li><li>1st session: 14:15-15:00</li><li>5-min break: 15:00-15:05</li><li>2nd session: 15:05-15:50</li></ul><h3 id=tutorial-times>Tutorial times</h3><ul><li>Fridays 16:00-18:00</li><li>Up to discussion</li></ul><h3 id=exam-dates>Exam dates</h3><ul><li>Main exam: 21/02/2023 (tentative)</li><li>Make-up exam: 11/04/2023 (tentative)</li></ul><h3 id=schedule--exercises>Schedule & exercises</h3><table class="table table-striped table-bordered"><thead><tr><th>#</th><th>Date</th><th>Content</th><th>Exercises</th><th>Lead tutor</th></tr></thead><tbody><tr><td>L1</td><td>21/10/2022</td><td>Introduction</td><td><a href=TBD>Exercise 0</a> (Due: 28/10/2022)</td><td>Alex</td></tr><tr><td>L2</td><td>28/10/2022</td><td>OOD Generalisation - Definition and limitations</td><td><a href=TBD>Exercise 1</a> (Due: 17/11/2022)</td><td>Alex</td></tr><tr><td>L3</td><td>04/11/2022</td><td>OOD Generalisation - Cue selection problem</td><td></td><td>Alex</td></tr><tr><td>L4</td><td>11/11/2022</td><td>OOD Generalisation - Worst-case generalisation</td><td></td><td>Alex</td></tr><tr><td>L5</td><td>18/11/2022</td><td>Explainability - Definition and limitations</td><td><a href=TBD>Exercise 2</a> (Due: 08/12/2022)</td><td>Elisa</td></tr><tr><td>L6</td><td>25/11/2022</td><td>Explainability - Attribution</td><td></td><td>Elisa</td></tr><tr><td>L7</td><td>02/12/2022</td><td>Explainability - Model explanation</td><td></td><td>Elisa</td></tr><tr><td>L8</td><td>09/12/2022</td><td>Uncertainty - Definition and evaluation</td><td><a href=TBD>Exercise 3</a> (Due: 26/01/2023)</td><td>Elif</td></tr><tr><td></td><td></td><td>Christmas Break</td><td></td><td></td></tr><tr><td>L9</td><td>13/01/2023</td><td>Uncertainty - Epistemic uncertainty</td><td></td><td>Elif</td></tr><tr><td>L10</td><td>20/01/2023</td><td>Uncertainty - Aleatoric uncertainty</td><td></td><td>Elif</td></tr><tr><td>L11</td><td>27/01/2023</td><td>Evaluation - Concepts and definitions</td><td><a href=TBD>Exercise 4</a> (Due: 09/02/2023)</td><td>Elisa</td></tr><tr><td>L12</td><td>03/02/2023</td><td>Evaluation - Weak & strong supervision</td><td></td><td>Elisa</td></tr></tbody></table></div></div></div></div><footer class=site-footer><div class=container><p class=powered-by>© Seong Joon Oh, 2022 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js></script>
<script src=//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js></script>
<script type=text/x-mathjax-config>
 MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script></body></html>