<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7RQJFKWW"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RF7RQJFKWW")</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script>
<script src=https://ScalableTrustworthyAI.github.io/js/hugo-academic-group.js></script>
<link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/bootstrap.min.css><script src=https://ScalableTrustworthyAI.github.io/js/bootstrap.min.js></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css><script src=https://ScalableTrustworthyAI.github.io/js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/font-awesome.min.css><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/hugo-academic-group.css><link rel="shortcut icon" href=https://ScalableTrustworthyAI.github.io/img/favicon.png type=image/x-icon><link rel=canonical href=https://ScalableTrustworthyAI.github.io/publication/nam2022iccv/><title>Scratching Visual Transformer's Back with Uniform Attention | Scalable Trustworthy AI</title></head><body><div class=home-anchor id=home></div><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://ScalableTrustworthyAI.github.io/><img src=https://ScalableTrustworthyAI.github.io/img/stai_logo.png alt="Research group logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#top>Home</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#overview>Overview</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#members>Members</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#courses>Courses</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#openings>Openings</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#contact>Contact</a></li></ul></div></div></nav><div class=container><div class=pub itemscope itemtype=http://schema.org/CreativeWork><div class=row><div class=col-sm-12><div class=pub-title><h1 itemprop=name>Scratching Visual Transformer's Back with Uniform Attention</h1></div></div></div><div class=row><div class=col-sm-12><div class=pub-authors itemprop=author><div itemprop=author><span class=author-name>Hyeon-Woo Nam,</span>
<span class=author-name>Yu-Ji Kim,</span>
<span class=author-name>Byeongho Heo,</span>
<span class=author-name>Dongyoon Han,</span>
<span class=author-name><a href=https://ScalableTrustworthyAI.github.io/member/joon/>Seong Joon Oh</a>,</span>
<span class=author-name>Tae-Hyun Oh</span></div></div></div></div><img src=https://ScalableTrustworthyAI.github.ioimg/nam2022iccv.png class=pub-banner itemprop=image><div class=row><div class=col-sm-12><h3>Abstract</h3></div></div><div class=row><div class=col-sm-12><p class=pub-abstract itemprop=text>The favorable performance of Vision Transformers (ViTs) is often attributed to the multi-head self-attention (MSA). The MSA enables global interactions at each layer of a ViT model, which is a contrasting feature against Convolutional Neural Networks (CNNs) that gradually increase the range of interaction across multiple layers. We study the role of the density of the attention. Our preliminary analyses suggest that the spatial interactions of attention maps are close to dense interactions rather than sparse ones. This is a curious phenomenon, as dense attention maps are harder for the model to learn due to steeper softmax gradients around them. We interpret this as a strong preference for ViT models to include dense interaction. We thus manually insert the uniform attention to each layer of ViT models to supply the much needed dense interactions. We call this method Context Broadcasting, CB. We observe that the inclusion of CB reduces the degree of density in the original attention maps and increases both the capacity and generalizability of the ViT models. CB incurs negligible costs: 1 line in your model code, no additional parameters, and minimal extra operations.</p></div></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div><div class="col-xs-12 col-sm-9">International Conference on Computer Vision</div></div></div></div><div class="visible-xs space-below"></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Date</div><div class="col-xs-12 col-sm-9" itemprop=datePublished>July, 2023</div></div></div></div><div class="visible-xs space-below"></div><div class=row style=padding-top:10px><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading" style=line-height:34px>Links</div><div class="col-xs-12 col-sm-9"><a class="btn btn-primary btn-outline" href=//arxiv.org/abs/2210.08457>PDF</a>
<a class="btn btn-primary btn-outline" href=//coallaoh.github.io/data/nam2022iccv.txt>Bibtex</a></div></div></div></div><div class="visible-xs space-below"></div><div class=space-below></div><div class=article-style></div></div></div><footer class=site-footer><div class=container><p class=powered-by>Â© Seong Joon Oh, 2023 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js></script>
<script src=//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js></script>
<script type=text/x-mathjax-config>
 MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script></body></html>