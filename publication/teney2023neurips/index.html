<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7RQJFKWW"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RF7RQJFKWW")</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script><script src=https://ScalableTrustworthyAI.github.io//js/hugo-academic-group.js></script><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io//css/bootstrap.min.css><script src=https://ScalableTrustworthyAI.github.io//js/bootstrap.min.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css><script src=https://ScalableTrustworthyAI.github.io//js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io//css/font-awesome.min.css><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io//css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io//css/hugo-academic-group.css><link rel="shortcut icon" href=https://ScalableTrustworthyAI.github.io//img/favicon.png type=image/x-icon><link rel=canonical href=https://ScalableTrustworthyAI.github.io/publication/teney2023neurips/><title>ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets | Scalable Trustworthy AI</title></head><body><div class=home-anchor id=home></div><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://ScalableTrustworthyAI.github.io/><img src=https://ScalableTrustworthyAI.github.io/img/stai_logo.png alt="Research group logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#top>Home</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#overview>Overview</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#members>Members</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#courses>Courses</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#openings>Openings</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#contact>Contact</a></li></ul></div></div></nav><div class=container><div class=pub itemscope itemtype=http://schema.org/CreativeWork><div class=row><div class=col-sm-12><div class=pub-title><h1 itemprop=name>ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets</h1></div></div></div><div class=row><div class=col-sm-12><div class=pub-authors itemprop=author><div itemprop=author><span class=author-name>Damien Teney,
</span><span class=author-name>Lin Yong,
</span><span class=author-name><a href=https://ScalableTrustworthyAI.github.io/member/joon/>Seong Joon Oh</a>,
</span><span class=author-name>Ehsan Abbasnejad</span></div></div></div></div><img src=https://ScalableTrustworthyAI.github.io/img/teney2023neurips.png class=pub-banner itemprop=image><div class=row><div class=col-sm-12><h3>Abstract</h3></div></div><div class=row><div class=col-sm-12><p class=pub-abstract itemprop=text>Several studies have empirically compared in-distribution (ID) and out-of-distribution (OOD) performance of various models. They report frequent positive correlations on benchmarks in computer vision and NLP. Surprisingly, they never observe inverse correlations suggesting necessary trade-offs. This matters to determine whether ID performance can serve as a proxy for OOD generalization. This short paper shows that inverse correlations between ID and OOD performance do happen in real-world benchmarks. They may have been missed in past studies because of a biased selection of models. We show an example of the pattern on the WILDS-Camelyon17 dataset, using models from multiple training epochs and random seeds. Our observations are particularly striking on models trained with a regularizer that diversifies the solutions to the ERM objective. We nuance recommendations and conclusions made in past studies. (1) High OOD performance does sometimes require trading off ID performance. (2) Focusing on ID performance alone may not lead to optimal OOD performance: it can lead to diminishing and eventually negative returns in OOD performance. (3) Our example reminds that empirical studies only chart regimes achievable with existing methods: care is warranted in deriving prescriptive recommendations.</p></div></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div><div class="col-xs-12 col-sm-9">Conference on Neural Information Processing Systems</div></div></div></div><div class="visible-xs space-below"></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Date</div><div class="col-xs-12 col-sm-9" itemprop=datePublished>To be published in
September, 2023</div></div></div></div><div class="visible-xs space-below"></div><div class=row style=padding-top:10px><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading" style=line-height:34px>Links</div><div class="col-xs-12 col-sm-9"><a class="btn btn-primary btn-outline" href=//arxiv.org/abs/2209.00613>PDF</a>
<a class="btn btn-primary btn-outline" href=//coallaoh.github.io/data/teney2023neurips.txt>Bibtex</a></div></div></div></div><div class="visible-xs space-below"></div><div class=space-below></div><div class=article-style></div></div></div><footer class=site-footer><div class=container><p class=powered-by>Â© Seong Joon Oh, 2024 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js></script><script type=text/x-mathjax-config>
 MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script></body></html>